{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TTT_RL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "fCv_ViDqFLLV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a Tic Tac Toe game were an AI uses Reinforcement Learning\n",
        "# CPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C6LO0z0CFfjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class Agent:\n",
        "  \n",
        "  def __init__(self, alpha = 0.5, epsilon = 0.1, verbose = True):\n",
        "    self.state_history = []\n",
        "    self.alpha = alpha\n",
        "    self.verbose = verbose\n",
        "    self.epsilon = epsilon\n",
        "    self.latest_pos = ()\n",
        "    self.attached = False\n",
        "    \n",
        "  def attach_to_env(self, env):\n",
        "    if self.attached == False:\n",
        "      self.env = env\n",
        "      self.V = np.zeros(self.env.num_of_sym**(self.env.board_dim[0]*self.env.board_dim[1]))\n",
        "    self.attached = True\n",
        " \n",
        "  def take_action(self, position):\n",
        "    self.env.place_move(self.sym, position)\n",
        "    self.latest_pos = position\n",
        "    \n",
        "  def undo_action(self):\n",
        "    self.env.delete_move(self.latest_pos)\n",
        "    \n",
        "  def random_action(self):\n",
        "    chosen_cell = random.choice(self.env.get_empty_cells())\n",
        "    self.take_action(chosen_cell)\n",
        "    \n",
        "  def strategic_action(self):\n",
        "    rand_num = random.random()\n",
        "    if rand_num < self.epsilon:\n",
        "      self.random_action()\n",
        "    else:\n",
        "      self.optimal_action()\n",
        "      \n",
        "  \n",
        "  def optimal_action(self):\n",
        "    empty_cells = self.env.get_empty_cells()\n",
        "    best_value = -1\n",
        "    chosen_cell = ()\n",
        "    for cell in empty_cells:\n",
        "      self.take_action(cell)\n",
        "      state, winner, ended = self.env.get_swe_tuple()   \n",
        "      self.undo_action()\n",
        "      value = self.V[state]\n",
        "      if value >= best_value:\n",
        "        best_value = value\n",
        "        chosen_cell = cell\n",
        "    self.take_action(chosen_cell)\n",
        "    \n",
        "  def assign_symbol(self, sym):\n",
        "    self.sym = sym\n",
        "    \n",
        "  def reset_state_history(self):\n",
        "    self.state_history = []\n",
        "  \n",
        "  # Create an array to map states to V value of the player\n",
        "  def initialize_v(self):\n",
        "    for state, winner, ended in self.env.swe_tuples:\n",
        "      if ended:        \n",
        "        if winner == self.sym:\n",
        "          v = 1.0\n",
        "        else:\n",
        "          v = 0.0\n",
        "      else:\n",
        "        v = 0.5\n",
        "      self.V[state] = v\n",
        "    # For debug  \n",
        "    #np.set_printoptions(threshold=np.nan)\n",
        "    #print(self.V)\n",
        "  \n",
        "  # Backtrack state history\n",
        "  # Vprev_state = Vprev_state + alpha * (Vnext-state - Vprev-state)   \n",
        "  # Where Vnext_state is the current state\n",
        "  # Will be executed after the end of an episode\n",
        "  def update_v(self):\n",
        "    self.update_state_history()\n",
        "    #print(self.state_history)\n",
        "    target = self.reward()\n",
        "    for prev in reversed(self.state_history):\n",
        "      value = self.V[prev] + self.alpha * (target - self.V[prev])\n",
        "      self.V[prev] = value\n",
        "      #print(\"v_update():{},{}\".format(prev,value))\n",
        "      target = value\n",
        "    self.reset_state_history()\n",
        "      \n",
        "  # Checks the reward at the end of the game\n",
        "  # Very important part in update_v()\n",
        "  def reward(self):\n",
        "    if not env.has_ended():\n",
        "      return 0\n",
        "    if self.sym == env.get_winner():\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  # Gets the state from the environment and update\n",
        "  # the agent's state history.\n",
        "  def update_state_history(self):\n",
        "    self.state_history = self.env.state_history\n",
        "    \n",
        "class Environment:\n",
        "   \n",
        "  def __init__(self, name, verbose = True):\n",
        "    self.name = name\n",
        "    self.board_dim = (3,3)\n",
        "    self.board_grid =  np.zeros(self.board_dim)\n",
        "    self.move_history = []\n",
        "    self.state_history = []\n",
        "    self.env_detail_history = []\n",
        "    self.num_of_sym = 3\n",
        "    self.player_sym = [\"X\", \"O\"]\n",
        "    self.board_sym = [\"X\", \"O\", \" \"]\n",
        "    self.coefficients = (-1,0,1)\n",
        "    self.swe_tuples = ()\n",
        "    self.verbose = verbose\n",
        "\n",
        "  # The order is important \n",
        "  # First player in the tuple is assigned to \"X\".\n",
        "  def register_players(self, players):\n",
        "    for i, player in enumerate(players):\n",
        "      player.attach_to_env(self)\n",
        "      player.assign_symbol(self.player_sym[i])\n",
        " \n",
        "  # Gets the winner by returning \"X\" or \"O\". Returns \"None\" otherwise.\n",
        "  def get_winner(self):\n",
        "    sums = []\n",
        "    sums.extend(self.board_grid.sum(0))  # sum of each columns\n",
        "    sums.extend(self.board_grid.sum(1))  # sum of each rows\n",
        "    sums.append(sum(self.board_grid.diagonal()))  # sum of top-left to bottom-right diagonal\n",
        "    sums.append(sum(np.fliplr(self.board_grid).diagonal()))  # sum of bottom-left to top-right diagonal\n",
        "    #print(sums)\n",
        "    \n",
        "    winner = []\n",
        "    if (3.0 in sums):\n",
        "      winner += \"O\"\n",
        "    if (-3.0 in sums):\n",
        "      winner += \"X\"\n",
        "    \n",
        "    if len(winner) == 1:\n",
        "      return winner[0]\n",
        "    else:\n",
        "      return \"None\"\n",
        " \n",
        "  def num_to_sym(self, num):\n",
        "    switcher = {\n",
        "        -1:\"X\",\n",
        "        0: \" \",\n",
        "        1: \"O\"\n",
        "    }\n",
        "    return switcher.get(num, \"invalid\")\n",
        "  \n",
        "  def sym_to_num(self, sym):\n",
        "    switcher = {\n",
        "        \"X\":-1,\n",
        "        \" \":0 ,\n",
        "        \"O\":1 \n",
        "    }\n",
        "    return switcher.get(sym, \"invalid\")\n",
        " \n",
        "  def place_move(self, sym, position):\n",
        "    env_detail = []\n",
        "    i,j = position\n",
        "    if self.board_grid[i,j] == 0:\n",
        "      self.board_grid[i,j] = self.sym_to_num(sym) \n",
        "       \n",
        "      env_detail.extend(self.get_swe_tuple())\n",
        "      env_detail.extend(sym)\n",
        "      env_detail.extend(position)\n",
        "      self.env_detail_history.append(env_detail)\n",
        "      \n",
        "      self.state_history.append(env_detail[0])\n",
        "      self.move_history.append(env_detail[3:6])            \n",
        "\n",
        "  \n",
        "  # Will undo the effects of place_move() and removes the record in move_history\n",
        "  # Needs to be used with precaution and should only be used after place_move()\n",
        "  def delete_move(self, position):\n",
        "    i,j = position\n",
        "    if self.board_grid[i,j] != 0:\n",
        "      self.board_grid[i,j] = 0 \n",
        "      del self.move_history[len(self.move_history)-1]\n",
        "      del self.state_history[len(self.state_history)-1]\n",
        "      del self.env_detail_history[len(self.env_detail_history)-1]\n",
        "      \n",
        "  \n",
        "  def check_cell_if_empty(self, position):\n",
        "    i,j = position\n",
        "    if self.board_grid[i,j] == 0:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "  def get_empty_cells(self):\n",
        "    cells_list = []\n",
        "    for i in range(self.board_dim[0]):\n",
        "      for j in range (self.board_dim[1]):\n",
        "        if self.check_cell_if_empty((i,j)) == True:\n",
        "          cells_list.append((i,j))\n",
        "    return cells_list\n",
        "     \n",
        "  def print_move_history(self):\n",
        "    print(\"Move History\")\n",
        "    for i, move in enumerate(self.move_history):\n",
        "      print(str(i) + \":\" + str(move))\n",
        "      \n",
        "  def print_state_history(self):\n",
        "    print(\"State History\")\n",
        "    for i, state in enumerate(self.state_history):\n",
        "      print(str(i) + \":\" + str(state))  \n",
        "\n",
        "  def print_env_detail_history(self):\n",
        "    print(\"Environment Detail History\")\n",
        "    for i, detail in enumerate(self.env_detail_history):\n",
        "      print(str(i) + \":\" + str(detail))  \n",
        "      \n",
        "  def print_env_winner(self):\n",
        "    winner = self.get_winner()\n",
        "    print(\"The winner is {}\".format(winner))\n",
        "  \n",
        "  def draw_board_in_sym(self):\n",
        "    print(\"---------\")\n",
        "    for i in range(0,self.board_dim[0]):\n",
        "      line_buff = \"\"\n",
        "      for j in range(0,self.board_dim[1]):\n",
        "        char_buff = \"|\" + self.num_to_sym(self.board_grid[i,j]) + \"|\"\n",
        "        line_buff += char_buff  \n",
        "      print(line_buff)\n",
        "      print(\"---------\")\n",
        "  \n",
        "  def draw_board_in_num(self):\n",
        "    print(self.board_grid)\n",
        "    \n",
        "  def has_ended(self): \n",
        "    if len(self.get_empty_cells()) == 0:\n",
        "      return True\n",
        "    else:\n",
        "      if self.get_winner() == \"None\":\n",
        "        return False\n",
        "      else:  \n",
        "        return True\n",
        "    \n",
        "  # Creates a hash from a given state. \n",
        "  # Based on converting the table into a decimal converted from base 3.\n",
        "  def get_state_in_hash(self):\n",
        "    # Coefficients taken from the board grid with these conditions\n",
        "    # -1 -> 0, 0 -> 1, 1 -> 2\n",
        "    coeff = np.ravel(self.board_grid + 1)\n",
        "    \n",
        "    # Hashing arithmetic\n",
        "    power = np.arange(self.board_dim[0]*self.board_dim[1])\n",
        "    base = np.full_like(coeff, self.num_of_sym)\n",
        "    \n",
        "    # hash taken from base3 to base10 conversion\n",
        "    return np.sum((coeff*(base**power)).astype(int))\n",
        "  \n",
        "  def get_swe_tuple(self):\n",
        "    state = self.get_state_in_hash()\n",
        "    pos = -1\n",
        "    for i, t in enumerate(env.swe_tuples):\n",
        "      if state in t:\n",
        "        pos = i\n",
        "    \n",
        "    if pos != -1:\n",
        "      return self.swe_tuples[pos]\n",
        "    else:\n",
        "      return None\n",
        "    \n",
        "  \n",
        "  \n",
        "  # Permutation of states in a recurrent from function\n",
        "  def recur_permutate_states(self, i=0, j=0):\n",
        "    results = []\n",
        "\n",
        "    for coeff in self.coefficients:\n",
        "      self.board_grid[i,j] = coeff\n",
        "      if j == self.board_dim[1]-1:\n",
        "        if i == self.board_dim[1]-1:\n",
        "          state_in_hash = self.get_state_in_hash()\n",
        "          winner = self.get_winner()\n",
        "          ended = self.has_ended()\n",
        "          results.append((state_in_hash, winner, ended))\n",
        "        else:\n",
        "          results += self.recur_permutate_states(i+1, 0)\n",
        "      else:\n",
        "        results += self.recur_permutate_states(i, j+1)   \n",
        "    return results\n",
        "  \n",
        "  # Permutate all state to create a look-up array and is\n",
        "  # saved to a instance variable\n",
        "  def permutate_states(self):\n",
        "    self.swe_tuples = self.recur_permutate_states()\n",
        "    self.board_grid.fill(0.0)\n",
        "    \n",
        "    if self.verbose:\n",
        "      print(\"Permutating all states/moves...\")   \n",
        "      print(self.swe_tuples)\n",
        "      \n",
        "class Human(Agent):\n",
        "\n",
        "  def strategic_action(self):\n",
        "    self.input_based_action()\n",
        "  \n",
        "  def input_based_action(self):\n",
        "    self.env.draw_board_in_sym()\n",
        "    posX = int(input())\n",
        "    posY = int(input())\n",
        "    position = (posX,posY)\n",
        "    self.env.place_move(self.sym, position)\n",
        "    self.latest_pos = position\n",
        "    \n",
        "  \n",
        "      \n",
        "# Loops until the game is over\n",
        "def play_game(env, players, verbose = False, display_board = False):\n",
        "  env.verbose = verbose\n",
        "  #env.register_players(random.sample(players, len(players)))\n",
        "  env.register_players(players)\n",
        "  env.permutate_states()\n",
        "  \n",
        "  for count, player in enumerate(players):\n",
        "    player.initialize_v() # initialize V values on each agent\n",
        "    if verbose:\n",
        "      print(\"Player %s is %s\" % (count, player.sym))\n",
        "  \n",
        "  while not env.has_ended():\n",
        "    for player in players:\n",
        "      player.strategic_action()\n",
        "      if env.has_ended():\n",
        "        break\n",
        "  \n",
        "  if display_board == True:\n",
        "    env.draw_board_in_sym()\n",
        "    env.print_env_winner()\n",
        " \n",
        "  for player in players:\n",
        "    player.update_v()\n",
        "\n",
        "    \n",
        "def train_agents(env, players, num_loop = 1):\n",
        "  \n",
        "  for i in range(num_loop):\n",
        "      play_game(env,players)\n",
        "      if i % 5 == 0:\n",
        "        print(i)\n",
        "\n",
        "  np.set_printoptions(threshold=np.nan)\n",
        "  for player in players:\n",
        "    print(player.V)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "o0EvDOtZyfGW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Choose between \"O\" or \"X\"\n",
        "pX = Agent()\n",
        "pO = Agent()\n",
        "pH = Human()\n",
        "\n",
        "env = Environment(\"TTT\")\n",
        "\n",
        "train_agents(env,(pX,pO),20)\n",
        "for i in range(5):\n",
        "  play_game(env,(pX,pH), display_board = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "37G4hgJrMnbl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a Tic Tac Toe game were an AI uses Reinforcement Learning\n",
        "# GPU\n",
        "!pip3 install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TNxcMQMPM5pU",
        "outputId": "81bf6a59-fa4a-452e-8384-d0b9905de5a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1700
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import torch \n",
        "import time\n",
        "\n",
        "\n",
        "# Acts as an abstract base class\n",
        "class Agent:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.attached = False\n",
        "  \n",
        "  def attach(self, env):\n",
        "    self.env = env\n",
        "    self.attached = True\n",
        "  \n",
        "  def do_action(self, action_info):\n",
        "    pass\n",
        "\n",
        "class Environment:\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def attach_agents(self, agents):\n",
        "    for agent in agents:\n",
        "      if agent.attached == False:\n",
        "        agent.attach(self)\n",
        "      \n",
        "  def visualize_world(self):\n",
        "    pass\n",
        "  \n",
        "  \n",
        "  \n",
        "# Derivative Classes  \n",
        "class AIplayer(Agent):\n",
        "  \n",
        "  def __init__(self, sym = \" \"):\n",
        "    super().__init__()\n",
        "    self.sym = sym\n",
        "    self.action_info = []\n",
        "    \n",
        "  def do_action(self, action_info):\n",
        "    ended = False\n",
        "    position = self.__random_position()\n",
        "    if position == None:\n",
        "      ended = True\n",
        "    action_info.extend((position, ended, self.sym))\n",
        "    \n",
        "  \n",
        "  def __place_sym_in_grid(self, pos):\n",
        "    self.env.grid_in_num[pos[0]][pos[1]] = self.__sym_to_num(self.sym)\n",
        "    \n",
        "  def __random_position(self):\n",
        "    empty_cells = self.__get_empty_cells()\n",
        "    if len(empty_cells) != 0:\n",
        "      chosen_cell = random.choice(empty_cells)\n",
        "      self.__place_sym_in_grid(chosen_cell) \n",
        "      return chosen_cell\n",
        "    else:\n",
        "      return None\n",
        "    \n",
        "  def __get_empty_cells(self):\n",
        "    grid = (self.env.grid_in_num == 0).nonzero()\n",
        "    empty_cells = []\n",
        "    for cell in grid.tolist():\n",
        "      empty_cells.append(cell)\n",
        "    return empty_cells\n",
        "  \n",
        "  def __sym_to_num(self, sym):\n",
        "    switcher = {\n",
        "        \"X\":-1,\n",
        "        \" \":0 ,\n",
        "        \"O\":1 \n",
        "    }\n",
        "    return switcher.get(sym, \"invalid\")\n",
        "  \n",
        "class TTTgame(Environment):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.__initialize_world()\n",
        "    self.player_sym = (\"X\",\"O\")\n",
        "  \n",
        "  def __initialize_world(self, dim = (3,3)):\n",
        "    self.grid_row = dim[0]\n",
        "    self.grid_col = dim[1]\n",
        "    self.grid_in_num = torch.zeros(dim[0],dim[1], dtype=torch.int)\n",
        "    \n",
        "  def get_winner(self):\n",
        "    sum_row = torch.sum(self.grid_in_num, 1)\n",
        "    sum_col = torch.sum(self.grid_in_num, 0)\n",
        "    print((sum_row,sum_col))\n",
        "    \n",
        "  def visualize_world(self):\n",
        "    self.__draw_grid_in_sym()\n",
        "       \n",
        "  def __draw_grid_in_sym(self):\n",
        "    print(\"---------\")\n",
        "    for i in range(0,self.grid_row):\n",
        "      line_buff = \"\"\n",
        "      for j in range(0,self.grid_col):\n",
        "        char_buff = \"|\" + self.__num_to_sym(self.grid_in_num[i][j].item()) + \"|\"\n",
        "        line_buff += char_buff  \n",
        "      print(line_buff)\n",
        "      print(\"---------\")\n",
        "      \n",
        "  def __num_to_sym(self, num):\n",
        "    switcher = {\n",
        "        -1:\"X\",\n",
        "        0: \" \",\n",
        "        1: \"O\"\n",
        "    }\n",
        "    return switcher.get(num, \"invalid\")\n",
        " \n",
        "  \n",
        "def play_game(players, game):\n",
        "  game.attach_agents(players)\n",
        "  game.visualize_world()\n",
        "  \n",
        "  # Assign players with symbols.\n",
        "  # Designed only for 2 players.\n",
        "  for i, player in enumerate(players):\n",
        "    player.sym = game.player_sym[i]\n",
        "  \n",
        "  # Game proper\n",
        "  info_list = []\n",
        "  ended = False\n",
        "  while not ended:\n",
        "    for player in players:\n",
        "      info = []\n",
        "      player.do_action(info)\n",
        "      game.visualize_world()\n",
        "      info_list.append(info)\n",
        "      print(info)\n",
        "      game.get_winner()\n",
        "      ended = info[1]\n",
        "      if ended == True:\n",
        "        break\n",
        "  print(info_list)\n",
        "      \n",
        "      \n",
        "   \n",
        "        \n",
        "      \n",
        "  \n",
        "    \n",
        "    \n",
        "# Start of the main function  \n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  \n",
        "  \n",
        "  pX = AIplayer()\n",
        "  pO = AIplayer()\n",
        "  #pH = TODO \n",
        "  \n",
        "  game = TTTgame()\n",
        "  \n",
        "  start = time.time()\n",
        "  play_game((pX,pO),game)\n",
        "  end = time.time()\n",
        "  print(end - start)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------\n",
            "| || || |\n",
            "---------\n",
            "| || || |\n",
            "---------\n",
            "| || || |\n",
            "---------\n",
            "---------\n",
            "| || ||X|\n",
            "---------\n",
            "| || || |\n",
            "---------\n",
            "| || || |\n",
            "---------\n",
            "[[0, 2], False, 'X']\n",
            "(tensor([-1,  0,  0]), tensor([ 0,  0, -1]))\n",
            "---------\n",
            "|O|| ||X|\n",
            "---------\n",
            "| || || |\n",
            "---------\n",
            "| || || |\n",
            "---------\n",
            "[[0, 0], False, 'O']\n",
            "(tensor([0, 0, 0]), tensor([ 1,  0, -1]))\n",
            "---------\n",
            "|O|| ||X|\n",
            "---------\n",
            "| ||X|| |\n",
            "---------\n",
            "| || || |\n",
            "---------\n",
            "[[1, 1], False, 'X']\n",
            "(tensor([ 0, -1,  0]), tensor([ 1, -1, -1]))\n",
            "---------\n",
            "|O|| ||X|\n",
            "---------\n",
            "| ||X|| |\n",
            "---------\n",
            "| || ||O|\n",
            "---------\n",
            "[[2, 2], False, 'O']\n",
            "(tensor([ 0, -1,  1]), tensor([ 1, -1,  0]))\n",
            "---------\n",
            "|O|| ||X|\n",
            "---------\n",
            "|X||X|| |\n",
            "---------\n",
            "| || ||O|\n",
            "---------\n",
            "[[1, 0], False, 'X']\n",
            "(tensor([ 0, -2,  1]), tensor([ 0, -1,  0]))\n",
            "---------\n",
            "|O|| ||X|\n",
            "---------\n",
            "|X||X|| |\n",
            "---------\n",
            "|O|| ||O|\n",
            "---------\n",
            "[[2, 0], False, 'O']\n",
            "(tensor([ 0, -2,  2]), tensor([ 1, -1,  0]))\n",
            "---------\n",
            "|O|| ||X|\n",
            "---------\n",
            "|X||X|| |\n",
            "---------\n",
            "|O||X||O|\n",
            "---------\n",
            "[[2, 1], False, 'X']\n",
            "(tensor([ 0, -2,  1]), tensor([ 1, -2,  0]))\n",
            "---------\n",
            "|O|| ||X|\n",
            "---------\n",
            "|X||X||O|\n",
            "---------\n",
            "|O||X||O|\n",
            "---------\n",
            "[[1, 2], False, 'O']\n",
            "(tensor([ 0, -1,  1]), tensor([ 1, -2,  1]))\n",
            "---------\n",
            "|O||X||X|\n",
            "---------\n",
            "|X||X||O|\n",
            "---------\n",
            "|O||X||O|\n",
            "---------\n",
            "[[0, 1], False, 'X']\n",
            "(tensor([-1, -1,  1]), tensor([ 1, -3,  1]))\n",
            "---------\n",
            "|O||X||X|\n",
            "---------\n",
            "|X||X||O|\n",
            "---------\n",
            "|O||X||O|\n",
            "---------\n",
            "[None, True, 'O']\n",
            "(tensor([-1, -1,  1]), tensor([ 1, -3,  1]))\n",
            "[[[0, 2], False, 'X'], [[0, 0], False, 'O'], [[1, 1], False, 'X'], [[2, 2], False, 'O'], [[1, 0], False, 'X'], [[2, 0], False, 'O'], [[2, 1], False, 'X'], [[1, 2], False, 'O'], [[0, 1], False, 'X'], [None, True, 'O']]\n",
            "0.026308059692382812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aB3O0o41NkMH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}