{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TTT_RL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "fCv_ViDqFLLV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a Tic Tac Toe game were an AI uses Reinforcement Learning"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C6LO0z0CFfjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class Agent:\n",
        "  \n",
        "  def __init__(self, alpha = 0.5, epsilon = 0.1, verbose = True):\n",
        "    self.state_history = []\n",
        "    self.alpha = alpha\n",
        "    self.verbose = verbose\n",
        "    self.epsilon = epsilon\n",
        "    self.latest_pos = ()\n",
        "    self.attached = False\n",
        "    \n",
        "  def attach_to_env(self, env):\n",
        "    if self.attached == False:\n",
        "      self.env = env\n",
        "      self.V = np.zeros(self.env.num_of_sym**(self.env.board_dim[0]*self.env.board_dim[1]))\n",
        "    self.attached = True\n",
        " \n",
        "  def take_action(self, position):\n",
        "    self.env.place_move(self.sym, position)\n",
        "    self.latest_pos = position\n",
        "    \n",
        "  def undo_action(self):\n",
        "    self.env.delete_move(self.latest_pos)\n",
        "    \n",
        "  def random_action(self):\n",
        "    chosen_cell = random.choice(self.env.get_empty_cells())\n",
        "    self.take_action(chosen_cell)\n",
        "    \n",
        "  def strategic_action(self):\n",
        "    rand_num = random.random()\n",
        "    if rand_num < self.epsilon:\n",
        "      self.random_action()\n",
        "    else:\n",
        "      self.optimal_action()\n",
        "      \n",
        "  \n",
        "  def optimal_action(self):\n",
        "    empty_cells = self.env.get_empty_cells()\n",
        "    best_value = -1\n",
        "    chosen_cell = ()\n",
        "    for cell in empty_cells:\n",
        "      self.take_action(cell)\n",
        "      state, winner, ended = self.env.get_swe_tuple()   \n",
        "      self.undo_action()\n",
        "      value = self.V[state]\n",
        "      if value >= best_value:\n",
        "        best_value = value\n",
        "        chosen_cell = cell\n",
        "    self.take_action(chosen_cell)\n",
        "    \n",
        "  def assign_symbol(self, sym):\n",
        "    self.sym = sym\n",
        "    \n",
        "  def reset_state_history(self):\n",
        "    self.state_history = []\n",
        "  \n",
        "  # Create an array to map states to V value of the player\n",
        "  def initialize_v(self):\n",
        "    for state, winner, ended in self.env.swe_tuples:\n",
        "      if ended:        \n",
        "        if winner == self.sym:\n",
        "          v = 1.0\n",
        "        else:\n",
        "          v = 0.0\n",
        "      else:\n",
        "        v = 0.5\n",
        "      self.V[state] = v\n",
        "    # For debug  \n",
        "    #np.set_printoptions(threshold=np.nan)\n",
        "    #print(self.V)\n",
        "  \n",
        "  # Backtrack state history\n",
        "  # Vprev_state = Vprev_state + alpha * (Vnext-state - Vprev-state)   \n",
        "  # Where Vnext_state is the current state\n",
        "  # Will be executed after the end of an episode\n",
        "  def update_v(self):\n",
        "    self.update_state_history()\n",
        "    #print(self.state_history)\n",
        "    target = self.reward()\n",
        "    for prev in reversed(self.state_history):\n",
        "      value = self.V[prev] + self.alpha * (target - self.V[prev])\n",
        "      self.V[prev] = value\n",
        "      #print(\"v_update():{},{}\".format(prev,value))\n",
        "      target = value\n",
        "    self.reset_state_history()\n",
        "      \n",
        "  # Checks the reward at the end of the game\n",
        "  # Very important part in update_v()\n",
        "  def reward(self):\n",
        "    if not env.has_ended():\n",
        "      return 0\n",
        "    if self.sym == env.get_winner():\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  # Gets the state from the environment and update\n",
        "  # the agent's state history.\n",
        "  def update_state_history(self):\n",
        "    self.state_history = self.env.state_history\n",
        "    \n",
        "class Environment:\n",
        "   \n",
        "  def __init__(self, name, verbose = True):\n",
        "    self.name = name\n",
        "    self.board_dim = (3,3)\n",
        "    self.board_grid =  np.zeros(self.board_dim)\n",
        "    self.move_history = []\n",
        "    self.state_history = []\n",
        "    self.env_detail_history = []\n",
        "    self.num_of_sym = 3\n",
        "    self.player_sym = [\"X\", \"O\"]\n",
        "    self.board_sym = [\"X\", \"O\", \" \"]\n",
        "    self.coefficients = (-1,0,1)\n",
        "    self.swe_tuples = ()\n",
        "    self.verbose = verbose\n",
        "\n",
        "  # The order is important \n",
        "  # First player in the tuple is assigned to \"X\".\n",
        "  def register_players(self, players):\n",
        "    for i, player in enumerate(players):\n",
        "      player.attach_to_env(self)\n",
        "      player.assign_symbol(self.player_sym[i])\n",
        " \n",
        "  # Gets the winner by returning \"X\" or \"O\". Returns \"None\" otherwise.\n",
        "  def get_winner(self):\n",
        "    sums = []\n",
        "    sums.extend(self.board_grid.sum(0))  # sum of each columns\n",
        "    sums.extend(self.board_grid.sum(1))  # sum of each rows\n",
        "    sums.append(sum(self.board_grid.diagonal()))  # sum of top-left to bottom-right diagonal\n",
        "    sums.append(sum(np.fliplr(self.board_grid).diagonal()))  # sum of bottom-left to top-right diagonal\n",
        "    #print(sums)\n",
        "    \n",
        "    winner = []\n",
        "    if (3.0 in sums):\n",
        "      winner += \"O\"\n",
        "    if (-3.0 in sums):\n",
        "      winner += \"X\"\n",
        "    \n",
        "    if len(winner) == 1:\n",
        "      return winner[0]\n",
        "    else:\n",
        "      return \"None\"\n",
        " \n",
        "  def num_to_sym(self, num):\n",
        "    switcher = {\n",
        "        -1:\"X\",\n",
        "        0: \" \",\n",
        "        1: \"O\"\n",
        "    }\n",
        "    return switcher.get(num, \"invalid\")\n",
        "  \n",
        "  def sym_to_num(self, sym):\n",
        "    switcher = {\n",
        "        \"X\":-1,\n",
        "        \" \":0 ,\n",
        "        \"O\":1 \n",
        "    }\n",
        "    return switcher.get(sym, \"invalid\")\n",
        " \n",
        "  def place_move(self, sym, position):\n",
        "    env_detail = []\n",
        "    i,j = position\n",
        "    if self.board_grid[i,j] == 0:\n",
        "      self.board_grid[i,j] = self.sym_to_num(sym) \n",
        "       \n",
        "      env_detail.extend(self.get_swe_tuple())\n",
        "      env_detail.extend(sym)\n",
        "      env_detail.extend(position)\n",
        "      self.env_detail_history.append(env_detail)\n",
        "      \n",
        "      self.state_history.append(env_detail[0])\n",
        "      self.move_history.append(env_detail[3:6])            \n",
        "\n",
        "  \n",
        "  # Will undo the effects of place_move() and removes the record in move_history\n",
        "  # Needs to be used with precaution and should only be used after place_move()\n",
        "  def delete_move(self, position):\n",
        "    i,j = position\n",
        "    if self.board_grid[i,j] != 0:\n",
        "      self.board_grid[i,j] = 0 \n",
        "      del self.move_history[len(self.move_history)-1]\n",
        "      del self.state_history[len(self.state_history)-1]\n",
        "      del self.env_detail_history[len(self.env_detail_history)-1]\n",
        "      \n",
        "  \n",
        "  def check_cell_if_empty(self, position):\n",
        "    i,j = position\n",
        "    if self.board_grid[i,j] == 0:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "  def get_empty_cells(self):\n",
        "    cells_list = []\n",
        "    for i in range(self.board_dim[0]):\n",
        "      for j in range (self.board_dim[1]):\n",
        "        if self.check_cell_if_empty((i,j)) == True:\n",
        "          cells_list.append((i,j))\n",
        "    return cells_list\n",
        "     \n",
        "  def print_move_history(self):\n",
        "    print(\"Move History\")\n",
        "    for i, move in enumerate(self.move_history):\n",
        "      print(str(i) + \":\" + str(move))\n",
        "      \n",
        "  def print_state_history(self):\n",
        "    print(\"State History\")\n",
        "    for i, state in enumerate(self.state_history):\n",
        "      print(str(i) + \":\" + str(state))  \n",
        "\n",
        "  def print_env_detail_history(self):\n",
        "    print(\"Environment Detail History\")\n",
        "    for i, detail in enumerate(self.env_detail_history):\n",
        "      print(str(i) + \":\" + str(detail))  \n",
        "      \n",
        "  def print_env_winner(self):\n",
        "    winner = self.get_winner()\n",
        "    print(\"The winner is {}\".format(winner))\n",
        "  \n",
        "  def draw_board_in_sym(self):\n",
        "    print(\"---------\")\n",
        "    for i in range(0,self.board_dim[0]):\n",
        "      line_buff = \"\"\n",
        "      for j in range(0,self.board_dim[1]):\n",
        "        char_buff = \"|\" + self.num_to_sym(self.board_grid[i,j]) + \"|\"\n",
        "        line_buff += char_buff  \n",
        "      print(line_buff)\n",
        "      print(\"---------\")\n",
        "  \n",
        "  def draw_board_in_num(self):\n",
        "    print(self.board_grid)\n",
        "    \n",
        "  def has_ended(self): \n",
        "    if len(self.get_empty_cells()) == 0:\n",
        "      return True\n",
        "    else:\n",
        "      if self.get_winner() == \"None\":\n",
        "        return False\n",
        "      else:  \n",
        "        return True\n",
        "    \n",
        "  # Creates a hash from a given state. \n",
        "  # Based on converting the table into a decimal converted from base 3.\n",
        "  def get_state_in_hash(self):\n",
        "    # Coefficients taken from the board grid with these conditions\n",
        "    # -1 -> 0, 0 -> 1, 1 -> 2\n",
        "    coeff = np.ravel(self.board_grid + 1)\n",
        "    \n",
        "    # Hashing arithmetic\n",
        "    power = np.arange(self.board_dim[0]*self.board_dim[1])\n",
        "    base = np.full_like(coeff, self.num_of_sym)\n",
        "    \n",
        "    # hash taken from base3 to base10 conversion\n",
        "    return np.sum((coeff*(base**power)).astype(int))\n",
        "  \n",
        "  def get_swe_tuple(self):\n",
        "    state = self.get_state_in_hash()\n",
        "    pos = -1\n",
        "    for i, t in enumerate(env.swe_tuples):\n",
        "      if state in t:\n",
        "        pos = i\n",
        "    \n",
        "    if pos != -1:\n",
        "      return self.swe_tuples[pos]\n",
        "    else:\n",
        "      return None\n",
        "    \n",
        "  \n",
        "  \n",
        "  # Permutation of states in a recurrent from function\n",
        "  def recur_permutate_states(self, i=0, j=0):\n",
        "    results = []\n",
        "\n",
        "    for coeff in self.coefficients:\n",
        "      self.board_grid[i,j] = coeff\n",
        "      if j == self.board_dim[1]-1:\n",
        "        if i == self.board_dim[1]-1:\n",
        "          state_in_hash = self.get_state_in_hash()\n",
        "          winner = self.get_winner()\n",
        "          ended = self.has_ended()\n",
        "          results.append((state_in_hash, winner, ended))\n",
        "        else:\n",
        "          results += self.recur_permutate_states(i+1, 0)\n",
        "      else:\n",
        "        results += self.recur_permutate_states(i, j+1)   \n",
        "    return results\n",
        "  \n",
        "  # Permutate all state to create a look-up array and is\n",
        "  # saved to a instance variable\n",
        "  def permutate_states(self):\n",
        "    self.swe_tuples = self.recur_permutate_states()\n",
        "    self.board_grid.fill(0.0)\n",
        "    \n",
        "    if self.verbose:\n",
        "      print(\"Permutating all states/moves...\")   \n",
        "      print(self.swe_tuples)\n",
        "      \n",
        "class Human(Agent):\n",
        "\n",
        "  def strategic_action(self):\n",
        "    self.input_based_action()\n",
        "  \n",
        "  def input_based_action(self):\n",
        "    self.env.draw_board_in_sym()\n",
        "    posX = int(input())\n",
        "    posY = int(input())\n",
        "    position = (posX,posY)\n",
        "    self.env.place_move(self.sym, position)\n",
        "    self.latest_pos = position\n",
        "    \n",
        "  \n",
        "      \n",
        "# Loops until the game is over\n",
        "def play_game(env, players):\n",
        "  env.verbose = False\n",
        "  env.register_players(random.sample(players, len(players)))\n",
        "  env.permutate_states()\n",
        "  \n",
        "  for count, player in enumerate(players):\n",
        "    player.initialize_v() # initialize V values on each agent\n",
        "    print(\"Player %s is %s\" % (count, player.sym))\n",
        "  \n",
        "  while not env.has_ended():\n",
        "    for player in players:\n",
        "      player.strategic_action()\n",
        "      if env.has_ended():\n",
        "        break\n",
        "  \n",
        "  env.draw_board_in_sym()\n",
        "  env.print_env_winner()\n",
        " \n",
        "  for player in players:\n",
        "    player.update_v()\n",
        "\n",
        "    \n",
        "def train_agents(env, players, num_loop = 1):\n",
        "  \n",
        "  for i in range(num_loop):\n",
        "      play_game(env,players)\n",
        "\n",
        "  np.set_printoptions(threshold=np.nan)\n",
        "  for player in players:\n",
        "    print(player.V)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "o0EvDOtZyfGW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Choose between \"O\" or \"X\"\n",
        "p1 = Agent()\n",
        "p2 = Agent()\n",
        "pH = Human()\n",
        "\n",
        "env = Environment(\"TTT\")\n",
        "\n",
        "train_agents(env,(p1,p2),100)\n",
        "for i in range(5):\n",
        "  play_game(env,(p1,pH))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PjaLrCwEWkWE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}